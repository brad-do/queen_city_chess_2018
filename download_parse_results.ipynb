{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://ccpf.proscan.com/programs/queen-city-classic-chess-tournament/'><img src='https://ccpf.proscan.com/wp-content/themes/proscanfund/library/images/sidebar-queen-city-classic-chess-tournament.png'/></a>\n",
    "\n",
    "### On 3 March 2018, my child competed with close to 700 other students in the [Queen City Classic Chess Tournament](https://ccpf.proscan.com/programs/queen-city-classic-chess-tournament/).  It was a long day, but my boy and his school did well and seemed to enjoy themselves.  \n",
    "\n",
    "### The organizers graciously [publish the tournament results](https://ccpf.proscan.com/programs/queen-city-classic-chess-tournament/final-results/), albeit in PDF format.  Nevertheless, I thought this might make for a good opportunity to do a little bit of data analysis: what schools were most represented, grades, and so forth.  \n",
    "\n",
    "### The analysis will be done over two notebooks.  The first, this one, will focus on downloading, parsing the PDFs, performing some rudimentary cleaning, and then serializing the data into usable CSV files.  [The second](./analyze_tournament_data.ipynb) will do the analysis work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "import PyPDF2  # installation instructions here: https://anaconda.org/conda-forge/pypdf2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Scrape the tournament results page containing the links to all the individual result documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = requests.get(\"https://ccpf.proscan.com/programs/queen-city-classic-chess-tournament/final-results/\")\n",
    "soup = BeautifulSoup(result.content, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Grab the links to all the individual results documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_list = []\n",
    "results_section = soup.find(\"section\", class_=\"entry-content cf\")\n",
    "pdf_links = results_section.find_all(\"a\")\n",
    "for pdf_link in pdf_links:\n",
    "    title = pdf_link.text\n",
    "    link = pdf_link.attrs['href']\n",
    "    pdf_list.append([title, link])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Let's go ahead and save those result documents to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for p in pdf_list:\n",
    "    # save pdf to disk\n",
    "    r = requests.get(p[1])\n",
    "    pdf_file = p[1].split('/')[-1]\n",
    "    open(\"./data/\" + pdf_file, 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: For reference, let's also get the player lists and save them to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape the players list web page\n",
    "pl_result = requests.get(\"https://ccpf.proscan.com/programs/queen-city-classic-chess-tournament/player-list/\")\n",
    "pl_soup = BeautifulSoup(pl_result.content, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab all the hyperlinks to the various available pdfs that list the players\n",
    "pl_pdf_list = []\n",
    "results_section = pl_soup.find(\"section\", class_=\"entry-content cf\")\n",
    "pl_pdf_links = results_section.find_all(\"a\")\n",
    "for pl_pdf_link in pl_pdf_links:\n",
    "    title = pl_pdf_link.text\n",
    "    link = pl_pdf_link.attrs['href']\n",
    "    pl_pdf_list.append([title, link])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now, download the pdfs and save them locally\n",
    "for p in pl_pdf_list:\n",
    "    # save pdf to disk\n",
    "    r = requests.get(p[1])\n",
    "    pdf_file = p[1].split('/')[-1]\n",
    "    open(\"./data/\" + pdf_file, 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Parse the downloaded PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, here's my first problem: even though I've used [PyPDF2](https://pythonhosted.org/PyPDF2/) successfully in the past, the extractText method of PyPDF2 seems unable to parse these particular PDFs.  [The documentation says](https://pythonhosted.org/PyPDF2/PageObject.html#PyPDF2.pdf.PageObject):\n",
    " <blockquote>This [the extractText method] works well for some PDF files, but poorly for others, depending on the generator used.</blockquote>\n",
    "\n",
    "### So, I guess, PyPDF2 won't work for me here.  Alright, what's Plan B?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# My attempt to parse these PDFs with PyPDF2.  All I got was a bunch of blank text.\n",
    "\n",
    "# def load_pdf_into_dataframe(pdf_file, title):\n",
    "#     with open(pdf_file, \"rb\") as f:\n",
    "#         pdf_reader = PyPDF2.PdfFileReader(f, strict=False)\n",
    "        \n",
    "#         for i in range(pdf_reader.numPages):\n",
    "#             # iterate through the pages in the pdf\n",
    "#             pdf_page = pdf_reader.getPage(i)\n",
    "#             print(pdf_page.extractText())\n",
    "\n",
    "        \n",
    "# df = load_pdf_into_dataframe(\"./data/n1FINALIndiviual.pdf\", \"some title\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Plan B, let's see if the [Xpdf tools](https://www.xpdfreader.com/index.html) can help us parse the PDF files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, let's use the pdftotext utility to convert all the PDF documents to TXT documents that we can hopefully later parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdftotext = '\"C:\\\\Program Files (x86)\\\\xpdf-tools-win-4.00\\\\bin64\\\\pdftotext.exe\"'  # path to the util on my hard drive\n",
    "\n",
    "# loop through all the PDFs I downloaded and run the pdftotext utility against each of them\n",
    "for p in (pdf_list + pl_pdf_list):\n",
    "    pdf_path = './data/' + p[1].split('/')[-1]\n",
    "    txt_path = pdf_path.replace('.pdf', '.txt')\n",
    "    ! {pdftotext} -table {pdf_path} {txt_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, to pull the data into objects I can actually work with, numpy has this really interesting method [fromregex](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.fromregex.html).  If I'm clever enough to construct a regex that can match on each field of the data, the fromregex method can pull the data into a numpy array.  From my research, there appear to be three different table formats at work with this data:\n",
    "<ol>\n",
    "<li>A schema for Individual, non-ranked players</li>\n",
    "<li>A schema for Individual, ranked players</li>\n",
    "<li>And schema for team results, regardless of whether the players were ranked or not</li>\n",
    "</ol>\n",
    "#### So, with [a lot of testing](https://regex101.com/), I have devised three regular expressions for each of the three file schemas.  Now, I'll loop through the files and finally get some consolidated lists of this data.\n",
    "\n",
    "#### Quick update: I ran into a few problems with numpy's [fromregex](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.fromregex.html) method.  Most of the records parsed just find, but some garbage data still found its way into the lists; so, I'll just parse the files with the tried-and-true [re](https://docs.python.org/3/library/re.html) package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2018', '-- Kindergarten NonRated', 'Team Standings, Page 1', 'Plc', 'Code', 'Name (Players:Top 3 used)', 'Score', 'Med   Solk  SBx2  Cum'), ('1', 'Dragon', 'Dragon Chess Center (3)', '10.0', '20.5', '35.5', '33.0', '29.5'), ('2', 'DetCCC', 'Detroit City Chess Club (2)', '1.5', '14.5', '23.0', '1.5', '4.0')]\n",
      "\n",
      "\n",
      "[('2018', '-- Grade 2 NonRated', 'Team Standings,', 'Page', '1', 'Plc', 'Code', 'Name (Players:Top 3            used)                                 Score  Med   Solk  SBx2  Cum'), ('1', 'Dragon', 'Dragon Chess Center (3)', '12.5', '28.0', '47.0', '72.5', '40.0'), ('2', 'DetCCC', 'Detroit City Chess Club (5)', '9.0', '23.0', '40.0', '34.0', '28.5'), ('3', 'LincEl', 'Lincoln Elementary (5)', '7.5', '15.5', '27.5', '19.5', '16.0'), ('4', 'StIgna', 'St. Ignatius (2)', '4.5', '17.5', '29.5', '23.5', '15.5')]\n",
      "\n",
      "\n",
      "[('2018', '-- K-12 Open', 'Team Standings, Page', '1', 'Plc', 'Code', 'Name (Players:Top 3 used)', 'Score  Med   Solk  SBx2  Cum'), ('1', 'DetCCC', 'Detroit City Chess Club (16)', '11.5', '24.0', '41.5', '54.5', '34.0'), ('2', 'CCL', 'CCL (2)', '6.5', '18.0', '29.0', '35.5', '20.0'), ('3', 'Wellin', 'Wellington (5)', '5.5', '19.0', '32.0', '11.5', '14.5')]\n",
      "\n",
      "\n",
      "[('2018', '-- 4-6 OPEN', 'Team Standings, Page', '1', 'Plc', 'Code', 'Name (Players:Top 3 used)', 'Score  Med   Solk  SBx2  Cum'), ('1', 'Dragon', 'Dragon Chess Center (4)', '9.5', '22.5', '36.5', '39.5', '23.5'), ('2', 'DetCCC', 'Detroit City Chess Club (8)', '8.0', '22.0', '37.5', '26.5', '22.0'), ('3', 'GrtrCC', 'Greater Cincinnati Chinese Scho', '(3)', '6.5', '24.0', '41.5', '30.5  24.5'), ('4', 'Herita', 'Heritage Elementary (2)', '3.5', '15.0', '23.0', '12.5', '10.0')]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "re_ind_nr = r'\\s+([0-9]+)\\s+(.+?(?=\\s{2}))\\s{2,}(.+?(?=\\s{2}))\\s+(.+?(?=\\s{2}))\\s+(.+?(?=\\s{2}))\\s+(.+?(?=\\s{2}))\\s+(.+?(?=\\s{2}))\\s+(.+?(?=\\s{2}))\\s+(.+?(?=\\s{2}))\\s+(.+?(?=\\s{2}))\\s+(.+?(?=\\s{2}))\\s+(.+)'\n",
    "re_ind_r = r'\\s+([0-9]+)\\s+(.+?(?=\\s{2}))\\s{2,}(.+?(?=\\s{2}))\\s+([0-9 ]+?(?=\\s{2}))\\s+(.+?(?=\\s{2}))\\s+(.+?(?=\\s{2}))\\s+(.+?(?=\\s{2}))\\s+(.+?(?=\\s{2}))\\s+(.+?(?=\\s{2}))\\s+(.+?(?=\\s{2}))\\s+(.+?(?=\\s{2}))\\s+(.+?(?=\\s{2}))\\s+(.+)'\n",
    "re_team = r'([0-9]+)\\s+(.+?(?=\\s{2}))\\s+(.+?(?=\\s{2}))\\s+(.+?(?=\\s{2}))\\s+(.+?(?=\\s{2}))\\s+(.+?(?=\\s{2}))\\s+(.+?(?=\\s{2}))\\s+(.+)'\n",
    "# the regex still let's some garbage rows come through like headers and footers.  use this list to weed the garbage out\n",
    "elems_of_rows_to_remove = ['Score', 'Rnd1', 'Code', 'TBrk1']\n",
    "ind_nr_list = []\n",
    "ind_r_list = []\n",
    "team_list = []\n",
    "\n",
    "# iterate through the list of result files I downloaded.  The PDFs fall into one of three categories: team results, \n",
    "# ranked player results, or non-ranked player results.  The file names follow a loose convention: if \"team\" or \"tm\"\n",
    "# is in the file name, that file is a list of team results.  If a file name starts with \"n\", that file represents\n",
    "# results of non-ranked players.  All the rest are results of ranked players.\n",
    "for p in pdf_list:\n",
    "    title = p[0]\n",
    "    txt_file = './data/{0}'.format(p[1].split('/')[-1].replace('.pdf', '.txt'))\n",
    "    with open(txt_file, 'r') as f:\n",
    "        t = f.read()\n",
    "        if 'team' in title.lower() or 'tm' in title.lower():\n",
    "            l = re.findall(re_team, t)\n",
    "            l = [[title] + list(r) for r in l if not any(i in r for i in elems_of_rows_to_remove)]\n",
    "            [team_list.append(r) for r in l]\n",
    "        elif title.lower().startswith('n'):\n",
    "            l = re.findall(re_ind_nr, t)\n",
    "            l = [[title] + list(r) for r in l if not any(i in r for i in elems_of_rows_to_remove)]\n",
    "            [ind_nr_list.append(r) for r in l]\n",
    "        else:\n",
    "            l = re.findall(re_ind_r, t)\n",
    "            l = [[title] + list(r) for r in l if not any(i in r for i in elems_of_rows_to_remove)]\n",
    "            [ind_r_list.append(r) for r in l]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Success!  Let's take a quick gander at the shape of the three lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the teams list, there are 134 records.  The min record size is 9 and max is 9.\n",
      "For the Non-Ranked list, there are 468 records.  The min record size is 13 and max is 13.\n",
      "For the Ranked list, there are 208 records.  The min record size is 14 and max is 14.\n"
     ]
    }
   ],
   "source": [
    "print('For the teams list, there are {0} records.  The min record size is {1} and max is {2}.'.\n",
    "      format(len(team_list), len(max(team_list, key=len)), len(min(team_list, key=len))))\n",
    "\n",
    "print('For the Non-Ranked list, there are {0} records.  The min record size is {1} and max is {2}.'.\n",
    "      format(len(ind_nr_list), len(max(ind_nr_list, key=len)), len(min(ind_nr_list, key=len))))\n",
    "\n",
    "print('For the Ranked list, there are {0} records.  The min record size is {1} and max is {2}.'.\n",
    "      format(len(ind_r_list), len(max(ind_r_list, key=len)), len(min(ind_r_list, key=len))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let's also parse the players list just in case I need them later.  Looking at the two player [list](https://ccpf.proscan.com/wp-content/uploads/2018/03/Player-list-as-of-03.02.18_by-player-name.pdf) [files](https://ccpf.proscan.com/wp-content/uploads/2018/03/Player-list-as-of-03.02.18_by-team-name.pdf) on the site, you can see that they both contain the same data, they're just sorted differently.  So, I'm just going to parse one of the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_players = r'(.+?(?=\\s{2}))\\s{2,}(.+?(?=\\s{2}))\\s{2,}(.+?(?=\\s{2}))\\s{2,}(.+?(?=\\s{2}))\\s{2,}(.+?(?=\\s{2}))\\s{2,}(.+?(?=\\s{2}))'\n",
    "player_elems_to_remove = ['First Name']\n",
    "players_list = []\n",
    "\n",
    "# added on 17 Mar 2018, as the players list page now seems to be taken down\n",
    "pl_pdf_list = [['Player list as of 03.02.18 at noon_by player name', 'Player-list-as-of-03.02.18_by-player-name.pdf']]\n",
    "title = pl_pdf_list[0][0]\n",
    "\n",
    "txt_file = './data/{0}'.format(pl_pdf_list[0][1].split('/')[-1].replace('.pdf', '.txt'))\n",
    "with open(txt_file, 'r') as f:\n",
    "    t = f.read()\n",
    "    l = re.findall(re_players, t)\n",
    "    test = re.findall(re_players, t)\n",
    "    l = [[title] + list(r) for r in l if not any(i in r for i in player_elems_to_remove)]\n",
    "    [players_list.append(r) for r in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the Players list, there are 699 records.  The min record size is 7 and max is 7.\n",
      "\n",
      "Did you catch that?  699 students competed in this tournament!\n"
     ]
    }
   ],
   "source": [
    "print('For the Players list, there are {0} records.  The min record size is {1} and max is {2}.'.\n",
    "      format(len(players_list), len(max(players_list, key=len)), len(min(players_list, key=len))))\n",
    "\n",
    "print('\\nDid you catch that?  {0} students competed in this tournament!'.format(len(players_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Phew!  It took a lot of work to parse these PDF files.  At this point, I'll just save these lists to CSV and save the analysis for [another notebook](./analyze_tournament_data.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the team results to disk\n",
    "with open(\"./data/queen_city_2018_team_results.csv\", \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(team_list)\n",
    "    \n",
    "# save the ranked player results to disk\n",
    "with open(\"./data/queen_city_2018_ranked_player_results.csv\", \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(ind_r_list)\n",
    "    \n",
    "# save the non-ranked player results to disk\n",
    "with open(\"./data/queen_city_2018_non_ranked_player_results.csv\", \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(ind_nr_list)\n",
    "    \n",
    "# save the player list to disk\n",
    "with open(\"./data/queen_city_2018_registered_players.csv\", \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(players_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
